# Diction-Analysis-of-the-Stock-Market with Predictions ‚Äî Grant Dries

This project has two parts:  

1. **Scraper** ‚Äî Collects stock news from Finviz and Yahoo Finance, scores headlines with VADER + keyword lookup, and measures price impact (+1h, +4h, EOW).  
   - This scrape takes a long time (~55 hours) since it pulls all headlines from the past 7 days.  
   - At the end it creates an Excel file.  

2. **Predictions** ‚Äî Uses that Excel file to generate future price predictions.  
   - This part runs quickly and outputs a new workbook with predictions added.  
TO RUN THIS CODE YOU MUST DOWNLOAD THE FINVIZ CSV FILE - THE CSV FILE CONTAINS THE TICKERS THE SCRAPER LOOKS FOR ON YAHOO FINANCE AND FINVIZ. LOOK TOWARDS BOTTOM FOR FILE SET UP - ITS UNDER RECOMENDED PROJECT SET UP!
---

## üì¶ Setup

Make sure you have Python 3.10+ installed. You‚Äôll also need to install some packages before running the scripts.

Install with pip:

```bash
pip install pandas numpy yfinance aiohttp beautifulsoup4 nltk python-dotenv tqdm openpyxl pytz python-dateutil

Or create a requirements.txt file with these lines and run:
pip install -r requirements.txt
pandas
numpy
yfinance
aiohttp
beautifulsoup4
nltk
python-dotenv
tqdm
openpyxl
pytz
python-dateutil

üìß Email Automation Setup (Scraper)

The scraper can automatically email you the weekly Excel report. To enable this:

Create a .env file in the same root directory as the scraper script.

Fill it in like this:
EMAIL_ADDRESS=INSERT_SENDER_EMAIL_ADDRESS_HERE
EMAIL_PASSWORD=INSERT_16_DIGIT_APP_PASSWORD
EMAIL_RECEIVER=INSERT_RECEIVING_EMAIL_ADDRESS_HERE

Important Notes

Do not use your normal Gmail password.
Generate a Gmail App Password:
Google Account ‚Üí Security ‚Üí App Passwords ‚Üí Generate New.
Paste the 16-character password into EMAIL_PASSWORD.
By default, the script uses Gmail (SMTP_SERVER and SMTP_PORT). If you want another provider, change those values in the script.

If the variables are missing, the script will skip email sending but still save the Excel file locally.

üìä Running the Scraper

Run the scraper script to collect headlines and create the weekly report:
This produces an Excel file:

weekly_sentiment_report.xlsx

With sheets:
Headlines ‚Äî all scraped headlines, scores, and price impact
Summary ‚Äî per-ticker averages (lookup score, VADER score, price changes, etc.)

üîÆ Running the Predictions Script

After you have the report, run the predictions script to add forecasts.

Make sure add_predictions.py is in the same root directory as:
The scraper script 
The Excel file from the scraper (weekly_sentiment_report.xlsx)

This creates:
weekly_sentiment_with_predictions_v4.xlsx

With sheets:
Headlines ‚Äî original + any intraday repairs
Summary ‚Äî adds a Pred_NextDay_% column
Predictions ‚Äî per-ticker forecast and latest headline info

üìÅ Recommended Project Layout
project-root/
‚îú‚îÄ weekly_sentiment_scraper.py
‚îú‚îÄ add_predictions.py
‚îú‚îÄ finviz.csv
‚îú‚îÄ .env                  # optional (for email sending)
‚îú‚îÄ requirements.txt
‚îî‚îÄ weekly_sentiment_report.xlsx  # created after scraper runs

‚ÑπÔ∏è Notes

The scraper is heavy and slow (multi-day run). The prediction step is fast.
If you don‚Äôt want to use email, skip the .env setup ‚Äî the Excel file still saves locally.

Good .gitignore additions:
.env
*.xlsx
__pycache__/
*.pyc

---
